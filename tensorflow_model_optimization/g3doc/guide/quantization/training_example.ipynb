{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Quantization aware training in Keras example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/model_optimization/guide/quantization/training_example\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/g3doc/guide/quantization/training_example.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/g3doc/guide/quantization/training_example.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/model-optimization/tensorflow_model_optimization/g3doc/guide/quantization/training_example.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjmi3qZeu_xk"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Welcome to an end-to-end example for *quantization aware training*.\n",
        "\n",
        "### Other pages\n",
        "For an introduction to what quantization aware training is and to determine if you should use it (including what's supported), see the [overview page](https://www.tensorflow.org/model_optimization/guide/quantization/training.md).\n",
        "\n",
        "To quickly find the APIs you need for your use case (beyond fully-quantizing a model with 8-bits), see the\n",
        "[comprehensive guide](https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide.md).\n",
        "\n",
        "### Summary\n",
        "\n",
        "In this tutorial, you will:\n",
        "\n",
        "1.   Train a `keras` model for MNIST from scratch.\n",
        "2.   Fine tune the model by applying the quantization aware training API, see the accuracy, and\n",
        "     export a quantization aware model.\n",
        "3.   Use the model to create an actually quantized model for the TFLite\n",
        "     backend.\n",
        "4.   See the persistence of accuracy in\n",
        "     TFLite and a 4x smaller model. To see the latency benefits on mobile, try out the TFLite examples [in the TFLite app repository](https://www.tensorflow.org/lite/models)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEAZYXvZU_XG"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zN4yVFK5-0Bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfa87e0c-21dd-4403-be45-82ee72a7e3d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.2 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install -q tensorflow\n",
        "! pip install -q tensorflow-model-optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yJwIonXEVJo6"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow_model_optimization.python.core.keras.compat import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip show tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vAnhmAJuLTM",
        "outputId": "9998b25f-ca29-442d-c3f2-b91b944c77ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: tensorflow\n",
            "Version: 2.17.1\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, requests, setuptools, six, tensorboard, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
            "Required-by: dopamine_rl, tf_keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install keras==2.11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "gNXmRrwpwZIk",
        "outputId": "e9093b0a-420f-47f2-e184-6640e18d6920"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras==2.11\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.7 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.5.0\n",
            "    Uninstalling keras-3.5.0:\n",
            "      Successfully uninstalled keras-3.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires keras>=3.2.0, but you have keras 2.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.11.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              },
              "id": "73004e48eb3c4778967c7328b57b8260"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psViY5PRDurp"
      },
      "source": [
        "## Train a model for MNIST without quantization aware training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbY-KGMPvbW9"
      },
      "outputs": [],
      "source": [
        "# # Load MNIST dataset\n",
        "# mnist = tf.keras.datasets.mnist\n",
        "# (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# # Normalize the input image so that each pixel value is between 0 to 1.\n",
        "# train_images = train_images / 255.0\n",
        "# test_images = test_images / 255.0\n",
        "\n",
        "# # Define the model architecture.\n",
        "# model = keras.Sequential([\n",
        "#   keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "#   keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "#   keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "#   keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "#   keras.layers.Flatten(),\n",
        "#   keras.layers.Dense(10)\n",
        "# ])\n",
        "\n",
        "# # Train the digit classification model\n",
        "# model.compile(optimizer='adam',\n",
        "#               loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# model.fit(\n",
        "#   train_images,\n",
        "#   train_labels,\n",
        "#   epochs=1,\n",
        "#   validation_split=0.1,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vsk5VXu0jyua",
        "outputId": "dc4cb902-2247-44ba-ab6c-035262d93ed4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/finalmodes/5devices_mobilenet_pos1\" ,compile=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwbC8-x_sdYd",
        "outputId": "176e8161-756e-4b25-875a-9d32c71fb54c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8747K9OE72P"
      },
      "source": [
        "## Clone and fine-tune pre-trained model with quantization aware training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F19k7ExXF_h2"
      },
      "source": [
        "### Define the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsZROpNYMWQ0"
      },
      "source": [
        "You will apply quantization aware training to the whole model and see this in the model summary. All layers are now prefixed by \"quant\".\n",
        "\n",
        "Note that the resulting model is quantization aware but not quantized (e.g. the weights are float32 instead of int8). The sections after show how to create a quantized model from the quantization aware one.\n",
        "\n",
        "In the [comprehensive guide](https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide.md), you can see how to quantize some layers for model accuracy improvements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oq6blGjgFDCW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c4fbeaf-9574-4bb3-8e59-614f7ca4d4f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 102, 22, 1)]      0         \n",
            "                                                                 \n",
            " quantize_layer (QuantizeLa  (None, 102, 22, 1)        3         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " quant_conv2d (QuantizeWrap  (None, 51, 11, 32)        385       \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_batch_normalization   (None, 51, 11, 32)        129       \n",
            " (QuantizeWrapperV2)                                             \n",
            "                                                                 \n",
            " quant_re_lu (QuantizeWrapp  (None, 51, 11, 32)        3         \n",
            " erV2)                                                           \n",
            "                                                                 \n",
            " quant_depthwise_conv2d (Qu  (None, 51, 11, 32)        323       \n",
            " antizeWrapperV2)                                                \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 51, 11, 32)        129       \n",
            " 1 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_re_lu_1 (QuantizeWra  (None, 51, 11, 32)        3         \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_conv2d_1 (QuantizeWr  (None, 51, 11, 64)        2241      \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 51, 11, 64)        257       \n",
            " 2 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_re_lu_2 (QuantizeWra  (None, 51, 11, 64)        3         \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_depthwise_conv2d_1 (  (None, 26, 6, 64)         643       \n",
            " QuantizeWrapperV2)                                              \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 26, 6, 64)         257       \n",
            " 3 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_re_lu_3 (QuantizeWra  (None, 26, 6, 64)         3         \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_conv2d_2 (QuantizeWr  (None, 26, 6, 128)        8577      \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 26, 6, 128)        513       \n",
            " 4 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_re_lu_4 (QuantizeWra  (None, 26, 6, 128)        3         \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_depthwise_conv2d_2 (  (None, 26, 6, 128)        1283      \n",
            " QuantizeWrapperV2)                                              \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 26, 6, 128)        513       \n",
            " 5 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_re_lu_5 (QuantizeWra  (None, 26, 6, 128)        3         \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_conv2d_3 (QuantizeWr  (None, 26, 6, 256)        33537     \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 26, 6, 256)        1025      \n",
            " 6 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_re_lu_6 (QuantizeWra  (None, 26, 6, 256)        3         \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_depthwise_conv2d_3 (  (None, 13, 3, 256)        2563      \n",
            " QuantizeWrapperV2)                                              \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 13, 3, 256)        1025      \n",
            " 7 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_re_lu_7 (QuantizeWra  (None, 13, 3, 256)        3         \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_conv2d_4 (QuantizeWr  (None, 13, 3, 512)        132609    \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 13, 3, 512)        2049      \n",
            " 8 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_re_lu_8 (QuantizeWra  (None, 13, 3, 512)        3         \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_depthwise_conv2d_4 (  (None, 7, 2, 512)         5123      \n",
            " QuantizeWrapperV2)                                              \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 7, 2, 512)         2049      \n",
            " 9 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_re_lu_9 (QuantizeWra  (None, 7, 2, 512)         3         \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_conv2d_5 (QuantizeWr  (None, 7, 2, 1024)        527361    \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 7, 2, 1024)        4097      \n",
            " 10 (QuantizeWrapperV2)                                          \n",
            "                                                                 \n",
            " quant_re_lu_10 (QuantizeWr  (None, 7, 2, 1024)        3         \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_depthwise_conv2d_5 (  (None, 7, 2, 1024)        10243     \n",
            " QuantizeWrapperV2)                                              \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 7, 2, 1024)        4097      \n",
            " 11 (QuantizeWrapperV2)                                          \n",
            "                                                                 \n",
            " quant_re_lu_11 (QuantizeWr  (None, 7, 2, 1024)        3         \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_conv2d_6 (QuantizeWr  (None, 7, 2, 1024)        1051649   \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 7, 2, 1024)        4097      \n",
            " 12 (QuantizeWrapperV2)                                          \n",
            "                                                                 \n",
            " quant_re_lu_12 (QuantizeWr  (None, 7, 2, 1024)        3         \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_max_pooling2d (Quant  (None, 7, 1, 1024)        1         \n",
            " izeWrapperV2)                                                   \n",
            "                                                                 \n",
            " quant_flatten (QuantizeWra  (None, 7168)              1         \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_dense (QuantizeWrapp  (None, 5)                 35850     \n",
            " erV2)                                                           \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1832668 (6.99 MB)\n",
            "Trainable params: 1816389 (6.93 MB)\n",
            "Non-trainable params: 16279 (63.59 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "# q_aware stands for for quantization aware.\n",
        "q_aware_model = quantize_model(model)\n",
        "\n",
        "# `quantize_model` requires a recompile.\n",
        "q_aware_model.compile(optimizer='adam',\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "q_aware_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path=\"/content/drive/MyDrive/Colab Notebooks\"\n",
        "os.chdir(path)\n",
        "os.listdir(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWuejhVw27hB",
        "outputId": "e0b2b67d-7dad-42d0-e707-7e8fbb7d9ab1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Untitled1.ipynb',\n",
              " '.ipynb_checkpoints',\n",
              " 'textcnn',\n",
              " 'unzipfile.ipynb',\n",
              " 'text-classification-cnn-rnn-master.zip',\n",
              " 'cnnBYpytorch.ipynb',\n",
              " 'text-classification-cnn-rnn-master',\n",
              " 'Test.ipynb',\n",
              " 'cnn-rnn',\n",
              " 'Untitled0.ipynb',\n",
              " 'finalmodes',\n",
              " '__pycache__',\n",
              " 'dataset_preparation.py']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDr2ijwpGCI-"
      },
      "source": [
        "### Train and evaluate the model against baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUBEn94hXYB1"
      },
      "source": [
        "To demonstrate fine tuning after training the model for just an epoch, fine tune with quantization aware training on a subset of the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_PHDGJryE31X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e89bc5b0-db31-4ccf-a66e-9cb30ac3af19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18000, 3180) (1, 18000)\n",
            "data shape label shape: (18000, 3180) (18000, 1)\n",
            "num_column:  22\n",
            "(18000, 102, 22, 1)\n",
            "data_train, data_valid, label_train, label_valid: (16200, 102, 22, 1) (1800, 102, 22, 1) (16200, 1) (1800, 1)\n"
          ]
        }
      ],
      "source": [
        "from dataset_preparation import awgn, LoadDataset, ChannelIndSpectrogram\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "LoadDatasetObj = LoadDataset()\n",
        "data, label=LoadDatasetObj.load_data_lable_train()\n",
        "print(\"data shape label shape:\",data.shape,label.shape)\n",
        "snr_range = np.arange(20,80)\n",
        "data = awgn(data, snr_range)\n",
        "\n",
        "ChannelIndSpectrogramObj = ChannelIndSpectrogram()\n",
        "\n",
        "# Convert time-domain IQ samples to channel-independent spectrograms.\n",
        "data = ChannelIndSpectrogramObj.channel_ind_spectrogram(data)\n",
        "batch_size = 32\n",
        "patience = 20\n",
        "\n",
        "\n",
        "early_stop = EarlyStopping('val_loss',\n",
        "                               min_delta = 0,\n",
        "                               patience =\n",
        "                               patience)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau('val_loss',\n",
        "                              min_delta = 0,\n",
        "                              factor = 0.2,\n",
        "                              patience = 10,\n",
        "                              verbose=1)\n",
        "callbacks = [early_stop, reduce_lr]\n",
        "\n",
        "# Split the dasetset into validation and training sets.\n",
        "data_train, data_valid, label_train, label_valid = train_test_split(data, label, test_size=0.1, shuffle= True)\n",
        "print(\"data_train, data_valid, label_train, label_valid:\",data_train.shape, data_valid.shape, label_train.shape, label_valid.shape)\n",
        "del data, label"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_aware_model.fit(x=data_train,y=label_train,steps_per_epoch = data_train.shape[0]//batch_size,\n",
        "                              epochs = 1,\n",
        "                              validation_data = (data_valid,label_valid),\n",
        "                              validation_steps = data_valid.shape[0]//batch_size,\n",
        "                              verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSn1kMg_8N-z",
        "outputId": "80aa3a69-fb29-40e4-9735-a2f0f53a0870"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tf_keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "491/506 [============================>.] - ETA: 1s - loss: 0.1013 - accuracy: 0.9831"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 506 batches). You may need to use the repeat() function when building your dataset.\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 56 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r506/506 [==============================] - 51s 73ms/step - loss: 0.1013 - accuracy: 0.9831 - val_loss: 0.0716 - val_accuracy: 0.9867\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x7df1827261a0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# q_aware_model.fit(train_images_subset, train_labels_subset,\n",
        "#                   batch_size=200, epochs=1, validation_split=0.1)"
      ],
      "metadata": {
        "id": "YLhzQ-8Xzq3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_enrol, true_label=LoadDatasetObj.load_iq_samples_test()\n",
        "ChannelIndSpectrogramObj = ChannelIndSpectrogram()\n",
        "data_enrol = ChannelIndSpectrogramObj.channel_ind_spectrogram(data_enrol)\n",
        ""
      ],
      "metadata": {
        "id": "oLpDU3RLlabe",
        "outputId": "7b68cf05-ea3c-44fb-dca6-82c62a615775",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6000, 3180) (1, 6000)\n",
            "num_column:  22\n",
            "(6000, 102, 22, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-byC2lYlMkfN"
      },
      "source": [
        "For this example, there is minimal to no loss in test accuracy after quantization aware training, compared to the baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6bMFTKSSHyyZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e1e1c79-6fa8-4b37-d798-25eb55026613"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quant test accuracy: 0.8884999752044678\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# _, baseline_model_accuracy = model.evaluate(\n",
        "#     data_enrol, true_label, verbose=0)\n",
        "\n",
        "_, q_aware_model_accuracy = q_aware_model.evaluate(\n",
        "   data_enrol, true_label, verbose=0)\n",
        "\n",
        "# print('Baseline test accuracy:', baseline_model_accuracy)\n",
        "print('Quant test accuracy:', q_aware_model_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IepmUPSITn6"
      },
      "source": [
        "## Create quantized model for TFLite backend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FgNP4rbOLH8"
      },
      "source": [
        "After this, you have an actually quantized model with int8 weights and uint8 activations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "w7fztWsAOHTz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8f9d524-81e5-46b8-f4af-c23252164b5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py:983: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "quantized_tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEYsyYVqNgeY"
      },
      "source": [
        "## See persistence of accuracy from TF to TFLite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saadXD4JQsBK"
      },
      "source": [
        "Define a helper function to evaluate the TF Lite model on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "b8yBouuGNqls"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_model(interpreter):\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on every image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  for i, test_image in enumerate(data_enrol):\n",
        "    if i % 1000 == 0:\n",
        "      print('Evaluated on {n} results so far.'.format(n=i))\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  print('\\n')\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  prediction_digits = np.array(prediction_digits)\n",
        "  accuracy = (prediction_digits == true_label).mean()\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuEFS4CIQvUw"
      },
      "source": [
        "You evaluate the quantized model and see that the accuracy from TensorFlow persists to the TFLite backend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VqQTyqz4NsWd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eaf8dc6-f409-42f0-896d-a0ea0dc8053e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluated on 0 results so far.\n",
            "Evaluated on 1000 results so far.\n",
            "Evaluated on 2000 results so far.\n",
            "Evaluated on 3000 results so far.\n",
            "Evaluated on 4000 results so far.\n",
            "Evaluated on 5000 results so far.\n",
            "\n",
            "\n",
            "Quant TFLite test_accuracy: 0.2000077222222222\n"
          ]
        }
      ],
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "test_accuracy = evaluate_model(interpreter)\n",
        "\n",
        "print('Quant TFLite test_accuracy:', test_accuracy)\n",
        "# print('Quant TF test accuracy:', q_aware_model_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8D7WnFF5DZR"
      },
      "source": [
        "## See 4x smaller model from quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1c2IecBRCdQ"
      },
      "source": [
        "You create a float TFLite model and then see that the quantized TFLite model\n",
        "is 4x smaller."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jy_Lgfh8VkyX"
      },
      "outputs": [],
      "source": [
        "# Create float TFLite model.\n",
        "float_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "float_tflite_model = float_converter.convert()\n",
        "\n",
        "# Measure sizes of models.\n",
        "_, float_file = tempfile.mkstemp('.tflite')\n",
        "_, quant_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "with open(quant_file, 'wb') as f:\n",
        "  f.write(quantized_tflite_model)\n",
        "\n",
        "with open(float_file, 'wb') as f:\n",
        "  f.write(float_tflite_model)\n",
        "\n",
        "print(\"Float model in Mb:\", os.path.getsize(float_file) / float(2**20))\n",
        "print(\"Quantized model in Mb:\", os.path.getsize(quant_file) / float(2**20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O5xuci-SonI"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2I7xmyMW5QY"
      },
      "source": [
        "In this tutorial, you saw how to create quantization aware models with the TensorFlow Model Optimization Toolkit API and then quantized models for the TFLite backend.\n",
        "\n",
        "You saw a 4x model size compression benefit for a model for MNIST, with minimal accuracy\n",
        "difference. To see the latency benefits on mobile, try out the TFLite examples [in the TFLite app repository](https://www.tensorflow.org/lite/models).\n",
        "\n",
        "We encourage you to try this new capability, which can be particularly important for deployment in resource-constrained environments.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "training_example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}